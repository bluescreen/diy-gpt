# Do it yourself GPT

GPT implementation using pytorch to understand the GPT Architecture

![GPT](gpt.png)

### Notes:

- *nn1.py* Basic 3 Layer neural net
- *nn2.py* Extract Model
- *nn3.py* Text Encoding/ Decoding
- *nn4.py* Embeddings
- *nn5.py* Attention Head 
- *nn6.py* Add Self Attention
- *main.py* Using Reweight
- *gpt.py* GPT Model Karpathy
### Terms:

- SGD = Stochastic Gradient Descent
- ADAM =  Adaptive Moment Estimation
- MSE = Mean Squared Error
- RELU = Rectified linear activation unit

### Papers:

[GPT-3 Paper, 2015](https://arxiv.org/pdf/2005.14165.pdf)

[Attention is all you need (GPT Architecture, 2017](https://arxiv.org/pdf/1706.03762.pdf)


### References:

[GPT Source from Andrey Karpathy](https://raw.githubusercontent.com/karpathy/ng-video-lecture/master/gpt.py)

[Nano GPT](https://github.com/karpathy/nanoGPT)

### Videos:

[From Zero To GPT & Beyond - Fast Paced Beginner Friendly Tutorial On Neural Networks](https://www.youtube.com/watch?v=l-CjXFmcVzY)

[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
